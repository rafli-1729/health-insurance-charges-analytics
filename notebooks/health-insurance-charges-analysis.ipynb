{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18513,"sourceType":"datasetVersion","datasetId":13720},{"sourceId":13840188,"sourceType":"datasetVersion","datasetId":8814882}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ“š 1. Standard Library and Configuration","metadata":{"_uuid":"e0c0b2fd-8eed-4e35-88e0-66bb4327d5d8","_cell_guid":"82c1d090-95f4-427e-ad78-ee947d35b7d8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# --- 1. SYSTEM & ENVIRONMENT CONFIGURATION ---\nimport os\nimport random\nfrom warnings import filterwarnings\n\n# Matikan log TensorFlow yang tidak perlu (Set sebelum import TF)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n# Abaikan warnings\nfilterwarnings(\"ignore\")\n\n\n# --- Core Library ---\nimport numpy as np\nimport pandas as pd\n\n\n# --- Visualization ---\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# --- Scikit Learn ---\n# Model Selection & Evaluation\nfrom sklearn.model_selection import (\n    KFold, \n    cross_validate, \n    cross_val_score, \n    train_test_split\n)\nfrom sklearn.metrics import (\n    r2_score, \n    mean_squared_error, \n    mean_absolute_error\n)\n\n# Preprocessing & Pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import (\n    OneHotEncoder, \n    StandardScaler, \n    RobustScaler\n)\n\n\n# --- Deep Learning Frameworks ---\n# TensorFlow & Keras\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (\n    Dense, \n    Dropout, \n    BatchNormalization,\n    Activation\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, \n    ReduceLROnPlateau\n)\n\n# ML Models\nimport optuna\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# --- Global Constant ---\nN_SPLITS = 5\nEPOCHS = 300\nBATCH_SIZE = 8\nTRAIN_PATH = '/kaggle/input/insurance/insurance.csv'\n\ndef set_seed(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    try:\n        tf.config.experimental.enable_op_determinism()\n    except AttributeError:\n        print(\"Warning: tf.config.experimental.enable_op_determinism() tidak tersedia di versi TF ini.\")\n\nSEED = 42\nset_seed(SEED)\n\nprint('Library and Configuration Ready!')","metadata":{"_uuid":"ae24f1ee-332c-4d22-a61d-05acd454e990","_cell_guid":"171584a0-dc43-412c-9021-c2b42ad1ab69","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-19T13:37:54.977966Z","iopub.execute_input":"2025-11-19T13:37:54.978262Z","iopub.status.idle":"2025-11-19T13:38:24.689356Z","shell.execute_reply.started":"2025-11-19T13:37:54.978238Z","shell.execute_reply":"2025-11-19T13:38:24.688258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ› ï¸ 2. Utility Functions & Modular Pipeline\n\nBagian ini berisi kumpulan fungsi modular yang dirancang untuk menyederhanakan proses *end-to-end* machine learning, mulai dari visualisasi data hingga *Deep Learning*.\n\n### 1. Feature Visualization (`plot_features`, `plot_numerical_correlation`)\nFungsi-fungsi ini digunakan untuk **Exploratory Data Analysis (EDA)**.\n* `plot_features`: Secara otomatis mendeteksi tipe data dan membuat Histogram (untuk numerik) serta Bar Plot (untuk kategorikal) dalam satu *grid layout* yang rapi.\n* `plot_numerical_correlation`: Menghasilkan *heatmap* korelasi Pearson untuk melihat hubungan antar fitur numerik.\n\n### 2. Preprocessing (`get_preprocessor`)\nMembangun `ColumnTransformer` standar menggunakan Scikit-Learn:\n* **Numerik**: Di-*scale* menggunakan `StandardScaler`.\n* **Kategorikal**: Di-*encode* menggunakan `OneHotEncoder`.\n\n### 3. Hyperparameter Tuning (`tune_model`, Search Spaces)\nMesin optimasi menggunakan **Optuna** untuk mencari parameter terbaik bagi model Tree-Based (XGBoost & LightGBM).\n* `get_xgb_search_space` & `get_lgbm_search_space`: Mendefinisikan rentang parameter yang akan diuji.\n* `tune_model`: Menjalankan *trial* Optuna dengan validasi silang (Cross-Validation) untuk meminimalkan MAE.\n\n### 4. Evaluation & Interpretation (`evaluate_final_model`, `plot_feature_importance`)\n* `evaluate_final_model`: Melakukan evaluasi akhir yang robust menggunakan **5-Fold Cross-Validation** dan menampilkan metrik R2, MAE, dan RMSE.\n* `plot_feature_importance`: Mengekstrak dan memvisualisasikan fitur mana yang paling berpengaruh terhadap prediksi model.\n\n### 5. Deep Neural Network Engine (`build_dnn_model`, `run_dnn_cv`)\nImplementasi khusus untuk Deep Learning menggunakan **Keras/TensorFlow**:\n* **Arsitektur**: Menggunakan layer `Dense`, `BatchNormalization`, dan `Dropout` untuk mencegah overfitting.\n* **Cross-Validation Manual**: `run_dnn_cv` menangani *looping* K-Fold secara manual agar preprocessing data (konversi ke numpy array) tidak bocor (*leakage*) antar fold.\n* **Permutation Importance**: `explain_dnn_feature_importance` menghitung *feature importance* untuk DNN dengan cara mengacak kolom fitur satu per satu dan melihat dampaknya terhadap *error*.","metadata":{}},{"cell_type":"code","source":"# --- To see the distribution for target features ---\ndef plot_target(series: pd.Series, log: bool = False) -> None:\n    \"\"\"\n    Visualizes the distribution of the target variable using a Histogram and KDE.\n    Optionally applies a log transformation to handle skewness.\n\n    Args:\n        series (pd.Series): The target variable data series (e.g., 'charges').\n        log (bool): If True, applies natural logarithm transformation (np.log) \n                    to the series before plotting. Defaults to False.\n    \"\"\"\n    # Apply log transformation if requested (useful for right-skewed data)\n    if log:\n        series = np.log(series)\n        \n    plt.figure(figsize=(15, 6))\n    \n    # Plot Histogram with Kernel Density Estimate (KDE)\n    sns.histplot(series, kde=True, bins=50, edgecolor='black')\n    \n    # Customize plot aesthetics\n    plt.grid(True, alpha=0.7)\n    \n    # Update title based on transformation status\n    title_suffix = \" (Log Transformed)\" if log else \"\"\n    plt.title(f'Target Distribution{title_suffix}', fontsize=20)\n    \n    plt.xlabel('Charges', fontsize=15)\n    plt.ylabel('Counts', fontsize=15)\n    \n    plt.tight_layout()\n    plt.show()\n\n# --- Feature Visualization Utility ---\n\ndef plot_features(X: pd.DataFrame, numerical_features: list, categorical_features: list) -> None:\n    \"\"\"\n    Generates histograms for numerical features and bar plots for categorical features\n    to visualize data distribution.\n\n    Args:\n        X (pd.DataFrame): Input dataframe containing the data.\n        numerical_features (list): List of column names corresponding to numerical data.\n        categorical_features (list): List of column names corresponding to categorical data.\n    \"\"\"\n    n_num = len(numerical_features)\n    n_cat = len(categorical_features)\n    \n    # Determine grid dimensions based on the number of features\n    max_cols = max(n_num, n_cat, 1)\n    \n    # Dynamic figure height: Allocate 4 units for each row (numerical/categorical)\n    fig_height = 4 * ((1 if n_num > 0 else 0) + (1 if n_cat > 0 else 0))\n    \n    fig = plt.figure(figsize=(max_cols * 5, fig_height))\n    gs = fig.add_gridspec(2, max_cols, hspace=0.4, wspace=0.3)\n    \n    # Plot Numerical Features\n    if n_num > 0:\n        print(f\"Plotting {n_num} numerical histograms...\")\n        for i, col in enumerate(numerical_features):\n            ax = fig.add_subplot(gs[0, i])\n            \n            # Limit bins to 50 or the number of unique values to prevent over-plotting\n            bins = min(X[col].nunique(), 50) \n            # Enable Kernel Density Estimate (KDE) only if there are enough unique values\n            kde = True if X[col].nunique() > 10 else False\n            \n            sns.histplot(X[col].dropna(), bins=bins, kde=kde, ax=ax, edgecolor='black')\n            \n            ax.set_title(f'Distribution: {col}', fontsize=12, fontweight='bold')\n            sns.despine(ax=ax) # Remove top and right spines for a cleaner look\n\n    # Plot Categorical Features\n    if n_cat > 0:\n        print(f\"Plotting {n_cat} categorical bar plots...\")\n        # If numerical features exist, plot categorical on the second row (index 1)\n        row_idx = 1 if n_num > 0 else 0 \n        \n        for i, col in enumerate(categorical_features):\n            ax = fig.add_subplot(gs[row_idx, i])\n            \n            display_counts = X[col].value_counts()\n            \n            sns.barplot(x=display_counts.index, y=display_counts.values, ax=ax, \n                        palette='viridis')\n            \n            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n            ax.set_title(f'Count: {col}', fontsize=12, fontweight='bold')\n            sns.despine(ax=ax)\n\n    plt.suptitle('Feature Distribution Visualization', fontsize=16, fontweight='heavy', y=0.98)\n    # Adjust subplots to fit into the figure area nicely\n    plt.tight_layout(rect=[0, 0, 1, 0.96]) \n    plt.show()\n\n\ndef plot_numerical_correlation(X: pd.DataFrame, numerical_features: list) -> None:\n    \"\"\"\n    Computes and visualizes the Pearson correlation matrix for numerical features\n    using a heatmap.\n\n    Args:\n        X (pd.DataFrame): Input dataframe.\n        numerical_features (list): List of numerical column names to correlate.\n    \"\"\"\n    # Calculate the pairwise correlation of columns, excluding NA/null values\n    correlation_matrix = X[numerical_features].corr(method='pearson')\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        correlation_matrix, \n        annot=True,           # Write the data value in each cell\n        fmt=\".2f\",            # String formatting code to use when adding annotations\n        cmap='coolwarm',      # Diverging colormap (Red for pos, Blue for neg)\n        vmin=-1, vmax=1,      # Anchor the colormap range\n        center=0,             # Center the colormap at 0\n        linewidths=.5,        \n        cbar_kws={'label': 'Correlation Coefficient'}\n    )\n    \n    plt.title('Numerical Features Correlation Heatmap (Pearson)', \n              fontsize=14, fontweight='bold')\n    \n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()\n\n# --- Preprocessing Utility ---\n\ndef get_preprocessor(num_cols: list, cat_cols: list) -> ColumnTransformer:\n    \"\"\"\n    Creates a sklearn ColumnTransformer to handle mixed types of data.\n    \n    Args:\n        num_cols (list): List of numerical column names.\n        cat_cols (list): List of categorical column names.\n        \n    Returns:\n        ColumnTransformer: A transformer object ready to be used in a pipeline.\n    \"\"\"\n    return ColumnTransformer(\n        transformers=[\n            # Apply OneHotEncoder to categorical cols; drop first to avoid dummy trap\n            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'), cat_cols),\n            # Apply StandardScaler to numerical cols for normalization\n            ('num', StandardScaler(), num_cols)\n        ], remainder='drop' # Drop any columns not specified in transformers\n    )\n\n# --- Search Space Definitions ---\n\ndef get_xgb_search_space(trial: optuna.Trial) -> dict:\n    \"\"\"\n    Defines the hyperparameter search space for XGBoost optimization.\n    \n    Args:\n        trial (optuna.Trial): The Optuna trial object used to suggest hyperparameters.\n        \n    Returns:\n        dict: A dictionary of hyperparameters for the XGBoost model.\n    \"\"\"\n    return {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 2000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        'n_jobs': -1, 'random_state': 42\n    }\n\ndef get_lgbm_search_space(trial: optuna.Trial) -> dict:\n    \"\"\"\n    Defines the hyperparameter search space for LightGBM optimization.\n    \n    Args:\n        trial (optuna.Trial): The Optuna trial object used to suggest hyperparameters.\n        \n    Returns:\n        dict: A dictionary of hyperparameters for the LightGBM model.\n    \"\"\"\n    return {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 50),\n        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        'verbosity': -1, 'n_jobs': 1, 'random_state': 42\n    }\n\n# --- Tuning Engine ---\n\ndef tune_model(model_cls, search_space_func, X, y, preprocessor, n_trials: int=50, study_name: str=\"optimization\") -> dict:\n    \"\"\"\n    Generic function to run Optuna hyperparameter tuning using Cross-Validation.\n\n    Args:\n        model_cls: The model class constructor (e.g., XGBRegressor).\n        search_space_func: Function that returns the parameter dictionary for a trial.\n        X, y: Training data features and target.\n        preprocessor: The sklearn preprocessor object/transformer.\n        n_trials (int): Number of optimization trials to run.\n        study_name (str): Name/Tag for the optuna study logging.\n        \n    Returns:\n        dict: The best hyperparameters found, combined with static parameters.\n    \"\"\"\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    print(f\"ðŸš€ Starting Tuning for {study_name}...\")\n\n    def objective(trial):\n        # Retrieve dynamic parameters from the search space function\n        params = search_space_func(trial)\n        # Instantiate the model with current trial parameters\n        model = model_cls(**params)        \n        \n        # Create a temporary pipeline for validation\n        pipeline = Pipeline([\n            ('preprocessor', preprocessor),\n            ('regressor', model)\n        ])\n        \n        # perform 3-fold CV to speed up the tuning process\n        scores = cross_val_score(pipeline, X, y, cv=3, \n                                 scoring='neg_mean_absolute_error', n_jobs=-1)\n        \n        # Optuna minimizes the objective, so we return negative MAE (to minimize error)\n        # Note: cross_val_score returns negative values for errors, so -scores.mean() is positive error.\n        # Actually, if direction is 'minimize', we should return the Error (positive).\n        # cross_val_score gives negative error (e.g. -5). -(-5) = 5. We want to minimize 5.\n        return -scores.mean()\n\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n    \n    print(f\"âœ… {study_name} Best MAE: {study.best_value:.4f}\")\n    \n    # Combine best params with static params (random_state, n_jobs) for final use\n    final_params = study.best_params.copy()\n    final_params.update({'random_state': 42, 'n_jobs': -1})\n    if study_name == \"LightGBM\": final_params['verbosity'] = -1\n        \n    return final_params\n\n# --- Evaluation Engine ---\n\ndef evaluate_final_model(model_cls, best_params: dict, X, y, preprocessor) -> Pipeline:\n    \"\"\"\n    Creates the final model pipeline with best parameters and runs robust \n    5-Fold Cross-Validation evaluation.\n\n    Args:\n        model_cls: The model class constructor.\n        best_params (dict): Dictionary of optimized hyperparameters.\n        X, y: Training data.\n        preprocessor: The sklearn preprocessor object.\n        \n    Returns:\n        Pipeline: The full, untrained pipeline object (ready for final fitting).\n    \"\"\"\n    \n    print(f\"\\nðŸ“Š EVALUATING FINAL MODEL: {model_cls.__name__}\")\n    \n    final_pipeline = Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', model_cls(**best_params))\n    ])\n    \n    # Perform 5-Fold CV with multiple scoring metrics\n    results = cross_validate(\n        final_pipeline, X, y, \n        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n        scoring={'r2': 'r2',\n                 'mae': 'neg_mean_absolute_error',\n                 'rmse': 'neg_root_mean_squared_error'},\n        n_jobs=1\n    )\n    \n    # Output scores for each fold\n    print(\"-\" * 60)\n    for i in range(len(results['test_r2'])):\n        r2 = results['test_r2'][i]\n        mae = -results['test_mae'][i]\n        rmse = -results['test_rmse'][i]\n        print(f\"  Fold {i+1} -> R2: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n\n    # Output the mean scores\n    print(\"-\" * 60)\n    print(f\"  AVG R2   : {results['test_r2'].mean():.4f}\")\n    print(f\"  AVG MAE  : {-results['test_mae'].mean():.2f}\")\n    print(f\"  AVG RMSE : {-results['test_rmse'].mean():.2f}\")\n    print(\"-\" * 60)\n    \n    return final_pipeline\n\ndef plot_feature_importance(pipeline: Pipeline, X, y, model_name: str=\"Model\") -> pd.DataFrame:\n    \"\"\"\n    Fits the pipeline to the full dataset, extracts feature importance scores, \n    and generates a bar plot.\n\n    Args:\n        pipeline (Pipeline): The full model pipeline (preprocessor + regressor).\n        X, y: Training data.\n        model_name (str): Name of the model for visualization titles.\n        \n    Returns:\n        pd.DataFrame: Dataframe containing feature names and their importance scores.\n    \"\"\"\n    print(f\"âš™ï¸ Sedang memproses Feature Importance untuk: {model_name}...\")\n    \n    # Fit Pipeline on the entire dataset to calculate final importances\n    pipeline.fit(X, y)\n    \n    # Access individual steps from the pipeline\n    model = pipeline.named_steps['regressor']\n    preprocessor = pipeline.named_steps['preprocessor']\n    \n    # Retrieve transformed feature names (handling OneHotEncoding prefixes)\n    raw_feature_names = preprocessor.get_feature_names_out()\n    # Clean the 'cat__' or 'num__' prefix added by ColumnTransformer\n    clean_feature_names = [name.split('__')[-1] for name in raw_feature_names]\n    \n    # Check if the model supports native feature importance\n    if hasattr(model, 'feature_importances_'):\n        importances = model.feature_importances_\n    else:\n        print(f\"âš ï¸ Model {model_name} tidak mendukung feature_importances_.\")\n        return None\n\n    # Create a DataFrame for easier plotting and sorting\n    importance_df = pd.DataFrame({\n        'feature': clean_feature_names,\n        'importance': importances\n    }).sort_values(by='importance', ascending=False)\n\n    # Visualization\n    plt.figure(figsize=(10, 6))\n    sns.barplot(\n        data=importance_df, # You can slice this to top N (e.g., importance_df.head(15))\n        x='importance', \n        y='feature',\n        palette='mako' # Elegant & deep palette for aesthetics\n    )\n    plt.title(f'Top Feature Importance - {model_name}', fontsize=14, fontweight='bold')\n    plt.xlabel('Importance Score')\n    plt.ylabel('Features')\n    plt.grid(axis='x', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    return importance_df\n\n\n# --- DNN Architecture Definition ---\n\ndef build_dnn_model(input_dim: int, learning_rate: float = 0.01) -> Sequential:\n    \"\"\"\n    Constructs and compiles a Keras Deep Neural Network architecture for regression.\n    \n    Args:\n        input_dim (int): Number of input features (after preprocessing).\n        learning_rate (float): Learning rate for the Adam optimizer.\n        \n    Returns:\n        Sequential: A compiled Keras model.\n    \"\"\"\n    model = Sequential([\n        # Layer 1: Input layer with 128 neurons\n        Dense(128, input_dim=input_dim),\n        BatchNormalization(), # Normalize inputs to speed up training\n        Activation('relu'),\n        Dropout(0.3), # Randomly drop 30% of neurons to prevent overfitting\n\n        # Layer 2: Hidden layer with 64 neurons\n        Dense(64),\n        BatchNormalization(),\n        Activation('relu'),\n        Dropout(0.2),\n\n        # Layer 3: Hidden layer with 16 neurons\n        Dense(16, activation='relu'),\n        \n        # Output Layer: Single neuron for regression output\n        Dense(1, activation='linear')\n    ])\n    \n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n    return model\n\n# --- DNN Cross-Validation Engine ---\n\ndef run_dnn_cv(X, y, num_cols, cat_cols, n_splits=5, epochs=100, batch_size=32):\n    \"\"\"\n    Executes K-Fold Cross-Validation specifically for the DNN model.\n    Note: Handles preprocessing inside the loop to prevent data leakage, as DNNs\n    require numpy arrays (not pandas DFs) as input.\n    \n    Args:\n        X, y: Input data features and target.\n        num_cols, cat_cols: Lists of feature names for preprocessing.\n        n_splits (int): Number of K-Fold splits.\n        epochs (int): Max training epochs per fold.\n        batch_size (int): Batch size for training.\n        \n    Returns:\n        dict: A dictionary containing average scores (R2, RMSE, MAE).\n    \"\"\"\n    print(f\"ðŸ§  Memulai {n_splits}-Fold Cross-Validation untuk DNN...\")\n    print(f\"   (Training | Max Epochs: {epochs} | Batch Size: {batch_size})\")\n    \n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    # Dictionary to store metrics from each fold\n    scores = {'r2': [], 'rmse': [], 'mae': []}\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n        # A. Split Data using indices\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # B. Preprocessing (Fresh instance per fold to avoid leakage)\n        fold_preprocessor = ColumnTransformer(\n            transformers=[\n                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'), cat_cols),\n                ('num', StandardScaler(), num_cols)\n            ], remainder='drop'\n        )\n        \n        # Fit on train, transform on val\n        X_train_proc = fold_preprocessor.fit_transform(X_train)\n        X_val_proc = fold_preprocessor.transform(X_val)\n        \n        # C. Build Model with correct input dimension\n        model = build_dnn_model(input_dim=X_train_proc.shape[1])\n        \n        # D. Define Callbacks for training stability\n        callbacks = [\n            # Stop training if validation loss stops improving\n            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0),\n            # Reduce learning rate when a metric has stopped improving\n            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0)\n        ]\n        \n        # E. Train the model (Silent mode)\n        model.fit(\n            X_train_proc, y_train,\n            validation_data=(X_val_proc, y_val),\n            epochs=epochs,\n            batch_size=batch_size,\n            callbacks=callbacks,\n            verbose=0 \n        )\n        \n        # F. Evaluate on Validation Set\n        y_pred = model.predict(X_val_proc, verbose=0).flatten()\n        \n        r2 = r2_score(y_val, y_pred)\n        rmse = mean_squared_error(y_val, y_pred, squared=False)\n        mae = mean_absolute_error(y_val, y_pred)\n        \n        scores['r2'].append(r2)\n        scores['rmse'].append(rmse)\n        scores['mae'].append(mae)\n        \n        print(f\"   âœ… Fold {fold+1}: R2={r2:.4f} | RMSE={rmse:.2f} | MAE={mae:.2f}\")\n        \n    # Calculate Averages across all folds\n    final_results = {\n        'R2 Score': np.mean(scores['r2']),\n        'RMSE': np.mean(scores['rmse']),\n        'MAE': np.mean(scores['mae'])\n    }\n    \n    print(\"\\n\" + \"=\"*50)\n    print(f\"RATA-RATA PERFORMA DNN ({n_splits} Folds)\")\n    print(\"=\"*50)\n    print(f\"R2   : {final_results['R2 Score']:.4f}\")\n    print(f\"MAE  : {final_results['MAE']:.4f}\")\n    print(f\"RMSE : {final_results['RMSE']:.4f}\")\n    print(\"=\"*50)\n    \n    return final_results\n\ndef train_final_dnn(X, y, num_cols, cat_cols, epochs=100, batch_size=32):\n    \"\"\"\n    Trains a single DNN model on the entire dataset for feature importance analysis.\n    \n    Returns:\n        tuple: (trained_keras_model, processed_features_numpy, feature_names_list)\n    \"\"\"\n    print(\"âš™ï¸ Melatih Final DNN pada seluruh dataset...\")\n    \n    # 1. Preprocessing\n    preprocessor = get_preprocessor(num_cols, cat_cols)\n    X_proc = preprocessor.fit_transform(X)\n    \n    # 2. Extract Feature Names from the preprocessor\n    raw_names = preprocessor.get_feature_names_out()\n    feature_names = [name.split('__')[-1] for name in raw_names]\n    \n    # 3. Build & Fit Model\n    model = build_dnn_model(input_dim=X_proc.shape[1])\n    model.fit(X_proc, y, epochs=epochs, batch_size=batch_size, verbose=0)\n    \n    print(\"âœ… Final DNN selesai dilatih.\")\n    return model, X_proc, feature_names\n\n# --- Visualization: DNN Permutation Importance ---\n\ndef explain_dnn_feature_importance(model, X_processed, y_true, feature_names):\n    \"\"\"\n    Computes and visualizes Feature Importance for Deep Learning models using\n    Permutation Importance (shuffling features one by one to measure error increase).\n    \n    Args:\n        model: Trained Keras model.\n        X_processed: Preprocessed input data (numpy array).\n        y_true: True target values.\n        feature_names: List of feature names corresponding to X_processed columns.\n        \n    Returns:\n        pd.DataFrame: Dataframe of feature importance scores.\n    \"\"\"\n    print(\"ðŸ” Menghitung DNN Feature Importance (ini mungkin memakan waktu)...\")\n    \n    # 1. Calculate Baseline MAE (Error with original data)\n    y_pred_base = model.predict(X_processed, verbose=0).flatten()\n    baseline_mae = mean_absolute_error(y_true, y_pred_base)\n    \n    importances = []\n    \n    # 2. Permutation Loop\n    # Iterate over each feature column to calculate its importance\n    for i in range(X_processed.shape[1]):\n        X_permuted = X_processed.copy()\n        np.random.shuffle(X_permuted[:, i]) # Randomly shuffle column i\n        \n        # Predict using the shuffled data\n        y_pred_perm = model.predict(X_permuted, verbose=0).flatten()\n        perm_mae = mean_absolute_error(y_true, y_pred_perm)\n        \n        # Importance = How much did the error increase compared to baseline?\n        importances.append(perm_mae - baseline_mae)\n        \n    # 3. Create DataFrame\n    df_imp = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance_MAE': importances\n    }).sort_values(by='Importance_MAE', ascending=False)\n    \n    # 4. Visualization\n    plt.figure(figsize=(10, 8))\n    sns.barplot(\n        data=df_imp,\n        x='Importance_MAE', \n        y='Feature',\n        palette='magma'\n    )\n    plt.title(f'DNN Feature Importance\\n(Baseline MAE: {baseline_mae:.2f})', fontsize=14, fontweight='bold')\n    plt.xlabel('Increase in MAE (Error) when feature is shuffled')\n    plt.ylabel('Features')\n    plt.grid(axis='x', linestyle='--', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    return df_imp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:38:24.691773Z","iopub.execute_input":"2025-11-19T13:38:24.692766Z","iopub.status.idle":"2025-11-19T13:38:24.745138Z","shell.execute_reply.started":"2025-11-19T13:38:24.692732Z","shell.execute_reply":"2025-11-19T13:38:24.743777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“Š 3. Load Data\n\n## Dataset Penelitian: Medical Cost Personal Dataset\n\nDataset yang digunakan dalam penelitian merupakan **Medical Cost Personal Dataset** yang diperoleh dari laman Kaggle (sumber: [Kaggle](https://www.kaggle.com/datasets/mirichoi0218/insurance/data)). Dataset ini berisi data individu yang diasuransikan, dengan tujuan untuk menganalisis faktor-faktor yang memengaruhi besarnya klaim medis (*medical charges*) yang ditanggung oleh perusahaan asuransi kesehatan.\n\nSecara keseluruhan, dataset ini terdiri atas **1.338 observasi** dan **7 variabel** yang mencakup karakteristik demografis, kondisi kesehatan, serta informasi biaya klaim. Adapun penjelasan tiap variabel adalah sebagai berikut.\n\n---\n\n### Deskripsi Variabel\n\n* **age**: Usia tertanggung utama (*primary beneficiary*) yang diasuransikan. Umur merupakan faktor risiko utama dalam klaim kesehatan karena berkorelasi positif dengan frekuensi dan keparahan klaim.\n* **sex**: Jenis kelamin tertanggung, terdiri atas kategori *female* dan *male*. Variabel ini digunakan untuk menilai adanya perbedaan biaya klaim antara laki-laki dan perempuan.\n* **bmi**: *Body Mass Index* (BMI), yaitu indeks massa tubuh yang dihitung dari rasio berat badan terhadap tinggi badan (kg/mÂ²). Nilai ideal berada pada kisaran 18,5â€“24,9. BMI digunakan sebagai indikator risiko kesehatan seperti obesitas yang berpotensi meningkatkan biaya klaim.\n* **children**: Jumlah anak atau tanggungan yang tercakup dalam polis asuransi kesehatan. Variabel ini menunjukkan beban keluarga dalam perlindungan asuransi.\n* **smoker**: Status kebiasaan merokok tertanggung (*yes* atau *no*). Faktor ini sangat berpengaruh terhadap besarnya premi maupun klaim karena berkaitan langsung dengan risiko penyakit kronis.\n* **region**: Wilayah tempat tinggal tertanggung di Amerika Serikat, dengan kategori *northeast*, *southeast*, *southwest*, dan *northwest*. Variabel ini dapat mencerminkan perbedaan biaya layanan kesehatan antar wilayah.\n* **charges**: Total biaya medis individual yang ditagihkan kepada perusahaan asuransi (*individual medical costs billed by health insurance*). Variabel ini menjadi variabel target (*dependent variable*) yang akan diprediksi dalam penelitian ini.\n\n---\n\nDengan karakteristik tersebut, dataset ini dinilai sesuai untuk mendukung analisis faktor-faktor yang memengaruhi besarnya klaim asuransi kesehatan menggunakan pendekatan *machine learning*. Selain itu, variabel-variabelnya memungkinkan untuk dilakukan interpretasi dari sudut pandang aktuaria, terutama dalam konteks *risk classification* dan *expected claim cost estimation*.","metadata":{"_uuid":"3c79d15f-694b-42f5-b173-9f715a3dd17c","_cell_guid":"6554c6e0-0a72-44d5-a5b9-68f5730f07d3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"insurance = pd.read_csv(TRAIN_PATH)\ninsurance.head()","metadata":{"_uuid":"5f4d1213-1a3e-457b-8710-893f09e0b115","_cell_guid":"049e005f-07b9-4dd8-b223-e0236281f8e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-19T13:38:24.746162Z","iopub.execute_input":"2025-11-19T13:38:24.746458Z","iopub.status.idle":"2025-11-19T13:38:24.857527Z","shell.execute_reply.started":"2025-11-19T13:38:24.746432Z","shell.execute_reply":"2025-11-19T13:38:24.856517Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Statistic descriptive of numerical features\\n')\nprint(insurance.describe())\n\nprint('\\nStatistic descriptive of categorical features\\n')\nprint(insurance.describe(include='object'))\n\nprint('\\nNumber of missing value each feature\\n')\nprint(insurance.isna().sum())","metadata":{"_uuid":"d5ed1673-e790-4e7d-a33f-8c84fc52e2b2","_cell_guid":"84ce4e19-1ceb-48d9-b093-8f666184d2cb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-19T13:38:24.85971Z","iopub.execute_input":"2025-11-19T13:38:24.860019Z","iopub.status.idle":"2025-11-19T13:38:24.897918Z","shell.execute_reply.started":"2025-11-19T13:38:24.859992Z","shell.execute_reply":"2025-11-19T13:38:24.896854Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check duplicates â†’ drop â†’ verify\nprint(\"Missing value before:\", insurance.duplicated().sum())\n\ninsurance.drop_duplicates(inplace=True)\nprint(\"Missing value after :\", insurance.duplicated().sum())","metadata":{"_uuid":"1442982e-ba76-4722-a2f3-3b6ac7a5d2ff","_cell_guid":"181ac439-f0ae-4183-97e1-a511f70a26ea","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-19T13:38:24.899095Z","iopub.execute_input":"2025-11-19T13:38:24.899391Z","iopub.status.idle":"2025-11-19T13:38:24.916713Z","shell.execute_reply.started":"2025-11-19T13:38:24.899367Z","shell.execute_reply":"2025-11-19T13:38:24.915407Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ”Ž 4. Exploratory Data Analysis","metadata":{"_uuid":"eff3017b-13df-4081-b196-194f3bdb3c72","_cell_guid":"35f02e3e-41b2-4734-bfe7-3e6f9ee0b3c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Devide datasets into two part, predictor and target\n\nX = insurance.drop(columns=['charges'])\ny = insurance.charges","metadata":{"_uuid":"98964fc4-a204-472f-ad2e-8416c523daef","_cell_guid":"38da67f7-22df-465a-aea1-0cb6899330ef","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-19T13:38:24.917705Z","iopub.execute_input":"2025-11-19T13:38:24.918028Z","iopub.status.idle":"2025-11-19T13:38:24.926391Z","shell.execute_reply.started":"2025-11-19T13:38:24.917997Z","shell.execute_reply":"2025-11-19T13:38:24.925361Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_target(y, log=False)\nplot_target(y, log=True)","metadata":{"_uuid":"f0373c9f-2427-4d76-aa22-7bb75af77910","_cell_guid":"0a44f9ee-4071-4fb0-b2d8-ff8f822712cc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-19T13:38:24.927366Z","iopub.execute_input":"2025-11-19T13:38:24.927633Z","iopub.status.idle":"2025-11-19T13:38:25.757906Z","shell.execute_reply.started":"2025-11-19T13:38:24.92761Z","shell.execute_reply":"2025-11-19T13:38:25.756918Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Distribusi charges menunjukkan pola yang sangat right-skewed, di mana sebagian besar individu membayar biaya asuransi dalam rentang rendah hingga menengah (sekitar di bawah 15.000), sementara hanya sebagian kecil yang memiliki biaya sangat tinggi hingga lebih dari 60.000. Ekor panjang di sisi kanan menandakan adanya outliers atau kelompok kecil dengan biaya medis ekstrem, yang kemungkinan besar dipengaruhi oleh faktor seperti status perokok atau kondisi kesehatan tertentu. Pola ini juga menunjukkan bahwa variabel target tidak berdistribusi normal, sehingga transformasi seperti log-transform dapat membantu model regresi untuk mempelajari pola dengan lebih stabil dan mengurangi efek ekstrem dari nilai-nilai tinggi.","metadata":{"_uuid":"3f3ee143-004b-4f6f-b863-744a69db3364","_cell_guid":"0fdc134c-3077-4291-a9d2-e279edbceefa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"numerical_features=X.select_dtypes(include=np.number).columns\ncategorical_features=X.select_dtypes(include='object').columns\n\nplot_features(X, numerical_features, categorical_features)","metadata":{"_uuid":"1b9e5f6c-903a-41b7-8eb6-c54dad507531","_cell_guid":"c72ead9c-8ea7-4a5c-a384-61ea0ed15712","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-19T13:38:25.758967Z","iopub.execute_input":"2025-11-19T13:38:25.759325Z","iopub.status.idle":"2025-11-19T13:38:27.321888Z","shell.execute_reply.started":"2025-11-19T13:38:25.759293Z","shell.execute_reply":"2025-11-19T13:38:27.320658Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Distribusi umur (age) tampak cukup merata dari usia 20 hingga 60 tahun tanpa pola khusus, menunjukkan tidak ada dominasi kelompok umur tertentu. Fitur BMI memiliki pola mendekati distribusi normal dengan pusat di sekitar 30, menunjukkan mayoritas peserta memiliki BMI overweight. Jumlah anak (children) didominasi oleh nilai 0â€“1, menandakan sebagian besar individu tidak memiliki anak atau hanya satu. Komposisi jenis kelamin (sex) relatif seimbang antara laki-laki dan perempuan. Status perokok (smoker) sangat tidak seimbang, dengan mayoritas besar adalah non-smoker. Distribusi wilayah (region) merata di empat region tanpa ada dominasi signifikan, sehingga tidak ada sampling bias besar dari wilayah.","metadata":{"_uuid":"e8485e97-0edc-4dde-9b45-8909f1848964","_cell_guid":"eb5a6a1b-d7f9-470a-b0a9-fc8f922666a3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"plot_numerical_correlation(X, numerical_features)","metadata":{"_uuid":"4e15b874-4e95-4277-bbca-c7be446fef26","_cell_guid":"a9d77384-5ac9-41c7-978e-4f98298f889d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-19T13:38:27.322956Z","iopub.execute_input":"2025-11-19T13:38:27.323254Z","iopub.status.idle":"2025-11-19T13:38:27.608871Z","shell.execute_reply.started":"2025-11-19T13:38:27.323225Z","shell.execute_reply":"2025-11-19T13:38:27.607848Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Heatmap menunjukkan bahwa hubungan antar fitur numerik (age, bmi, children) sangat lemah, dengan koefisien korelasi yang semuanya mendekati nol. Age hanya memiliki korelasi kecil dengan BMI, dan jumlah anak hampir tidak berhubungan dengan dua fitur lainnya. Ini mengindikasikan bahwa ketiga fitur tersebut cenderung berdiri sendiri dan tidak memiliki hubungan linear yang kuat satu sama lain, sehingga masing-masing kemungkinan berkontribusi secara independen terhadap variabel target (biaya insurance) dan tidak menimbulkan masalah multikolinearitas.","metadata":{"_uuid":"7fc77284-5e15-411b-a6e9-d5ba0c5b3e59","_cell_guid":"2d000d1a-259b-4535-95a1-d3e499ad42d6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# ðŸ“ˆ 5. Modeling","metadata":{"_uuid":"39dfd269-ef46-419c-a283-49d01e85ef7b","_cell_guid":"04a94176-e156-4771-ab3c-2c0d6210282a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"high_cardinality_cols = [features for features in categorical_features if X[features].nunique()>10]\nprint('High cardinality column: ', high_cardinality_cols)","metadata":{"_uuid":"87d98e3c-8771-4628-87cc-14bc8fbeb218","_cell_guid":"f6eab8b0-6286-4fab-8438-47f3c296cebd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-19T13:38:27.611632Z","iopub.execute_input":"2025-11-19T13:38:27.611942Z","iopub.status.idle":"2025-11-19T13:38:27.618676Z","shell.execute_reply.started":"2025-11-19T13:38:27.611919Z","shell.execute_reply":"2025-11-19T13:38:27.617741Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Semua variabel kategorikal tidak ada yang memiliki cardinality yang tinggi, semua variabel kategorik akan dilakukan One Hot Encoding","metadata":{"_uuid":"e8bbb6ba-3bd2-4e05-8071-9aef5bf18026","_cell_guid":"ac1486c4-8bd4-43a3-8896-33b3b2c9dada","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Feature Engineering\n\nUntuk membantu model menangkap pola risiko yang kompleks dan non-linier, kita membuat variabel-variabel turunan berikut:\n\n* **`bmi_encoded` (Kategori BMI)**\n    * **Tujuan:** Mengubah variabel numerik kontinu (`bmi`) menjadi variabel kategori ordinal (0, 1, 2, 3).\n    * **Penjelasan:** Hubungan BMI dengan biaya kesehatan seringkali bertingkat (*step-function*), bukan garis lurus. Pengelompokan ini membantu model membedakan risiko antara kategori berat badan (misal: *Underweight, Normal, Overweight, Obese*) secara lebih tegas.\n    * *Dibuat dari: `bmi`*\n\n* **`smoker_obese_interaction` (Interaksi Perokok & Obesitas)**\n    * **Tujuan:** Membuat fitur biner yang bernilai `1` jika seseorang adalah perokok **DAN** memiliki BMI tinggi (*Overweight/Obese* - kategori 2 atau 3).\n    * **Penjelasan:** Ini adalah \"Double Whammy Effect\". Dalam aktuaria, risiko kesehatan perokok yang juga obesitas seringkali **multiplikatif**, bukan aditif. Biaya mereka biasanya jauh lebih tinggi dibandingkan yang hanya perokok atau hanya obesitas.\n    * *Dibuat dari: `smoker` dan `bmi_encoded`*\n\n* **`has_dependent_or_high_risk` (Tanggungan atau Risiko Gaya Hidup)**\n    * **Tujuan:** Membuat fitur biner yang menandai jika seseorang punya anak **ATAU** merupakan perokok.\n    * **Penjelasan:** Menggabungkan dua faktor pemicu biaya berbeda: beban finansial keluarga (anak) atau beban risiko kesehatan (rokok). Ini mengelompokkan profil \"High Liability\" secara umum.\n    * *Dibuat dari: `children` dan `smoker`*\n\n* **`smoker_age_interaction` (Interaksi Usia Khusus Perokok)**\n    * **Tujuan:** Menyimpan nilai umur asli hanya jika orang tersebut perokok (jika tidak, nilainya 0).\n    * **Penjelasan:** Dampak penuaan pada tubuh perokok mungkin lebih merusak (akselerasi biaya) dibandingkan non-perokok. Fitur ini mengizinkan model memberikan \"bobot\" atau koefisien yang berbeda untuk umur pada kelompok perokok.\n    * *Dibuat dari: `smoker` dan `age`*\n\n* **`smoker_obese_aged_interaction` (Risiko Ekstrem: Tua, Merokok, Gemuk)**\n    * **Tujuan:** Fitur biner untuk segmen risiko sangat tinggi: Perokok + BMI Tinggi + Usia di atas 45 tahun.\n    * **Penjelasan:** Ini mengisolasi kelompok \"*Very High Risk*\". Seringkali model gagal memprediksi nilai ekstrem (*outlier*) yang sangat mahal. Fitur ini memberi sinyal khusus pada model bahwa kelompok ini kemungkinan besar memiliki klaim raksasa.\n    * *Dibuat dari: `smoker`, `bmi_encoded`, dan `age`*\n\n* **`bmi_smoker_continuous` (Interaksi BMI Kontinu Khusus Perokok)**\n    * **Tujuan:** Menyimpan nilai BMI asli hanya jika orang tersebut perokok.\n    * **Penjelasan:** Serupa dengan interaksi usia, kenaikan 1 poin BMI pada perokok mungkin berdampak lebih besar terhadap biaya daripada kenaikan 1 poin BMI pada non-perokok (efek sinergis pada jantung/paru).\n    * *Dibuat dari: `smoker` dan `bmi`*\n\n* **`age_sq` (Usia Kuadrat)**\n    * **Tujuan:** *Polynomial feature* untuk menangkap hubungan kuadratik pada usia.\n    * **Penjelasan:** Biaya kesehatan biasanya tidak naik secara linier seiring bertambahnya usia, melainkan melengkung ke atas (eksponensial/kuadratik) terutama di usia tua.\n    * *Dibuat dari: `age`*\n\n* **`bmi_sq` (BMI Kuadrat)**\n    * **Tujuan:** *Polynomial feature* untuk menangkap hubungan kuadratik pada BMI.\n    * **Penjelasan:** Risiko kesehatan mungkin naik drastis (kurva berbentuk J atau U) pada angka BMI yang sangat ekstrem.\n    * *Dibuat dari: `bmi`*\n\n* **`age_bmi_interaction` (Interaksi Usia dan BMI)**\n    * **Tujuan:** Perkalian antara usia dan BMI.\n    * **Penjelasan:** Dampak obesitas mungkin menjadi lebih parah seiring bertambahnya usia. Menjadi gemuk di usia 50 tahun mungkin membawa risiko komplikasi (dan biaya) yang lebih besar daripada gemuk di usia 20 tahun.\n    * *Dibuat dari: `age` dan `bmi`*\n\n* **`sex_smoker_interaction` (Interaksi Gender & Rokok)**\n    * **Tujuan:** Fitur biner khusus untuk Laki-laki yang Merokok.\n    * **Penjelasan:** Menguji hipotesis apakah ada perbedaan pola klaim antara perokok pria dan wanita (misalnya karena perbedaan intensitas merokok atau kerentanan biologis).\n    * *Dibuat dari: `sex` dan `smoker`*\n\n* **`non_smoker_high_risk` (Risiko Tinggi Non-Perokok)**\n    * **Tujuan:** Mengidentifikasi orang yang **bukan** perokok, tapi Tua (>45) dan Berat Badan Berlebih.\n    * **Penjelasan:** Menangkap segmen risiko murni dari faktor metabolik dan penuaan tanpa pengaruh rokok. Ini membantu model membedakan sumber risiko biaya.\n    * *Dibuat dari: `smoker`, `bmi_encoded`, dan `age`*\n\n* **`log_charges` (Transformasi Log pada Target)**\n    * **Tujuan:** Membuat variabel target baru dengan menerapkan transformasi logaritma natural (`np.log`) pada variabel `charges` asli.\n    * **Penjelasan:** Distribusi data `charges` asli sangat miring ke kanan (*right-skewed*). Distribusi yang miring ini melanggar asumsi banyak model dan dapat menurunkan akurasi. `log_charges` membuat distribusi target lebih normal (simetris).\n    * *Dibuat dari: `charges`*","metadata":{"_uuid":"d40c2c18-96f9-4411-8d4e-6bbebda3375e","_cell_guid":"0c46d3f5-63ea-4d93-8526-22412fb96dce","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"bins = [-np.inf, 18.5, 25, 30, np.inf]\nlabels = [0, 1, 2, 3]\n\ninsurance['bmi_encoded'] = pd.cut(insurance['bmi'], bins=bins, labels=labels)\n\ninsurance['smoker_obese_interaction'] = np.where(\n    (insurance['smoker'] == 'yes') & (insurance['bmi_encoded'].isin([2, 3])), \n    1, 0\n)\n\ninsurance['has_dependent_or_high_risk'] = np.where(\n    (insurance['children'] > 0) | (insurance['smoker'] == 'yes'), \n    1, 0\n)\n\n\ninsurance['smoker_age_interaction'] = np.where(\n    insurance['smoker'] == 'yes',\n    insurance['age'],\n    0\n)\n\ninsurance['smoker_obese_aged_interaction'] = np.where(\n    (insurance['smoker'] == 'yes') & \n    (insurance['bmi_encoded'].isin([2, 3])) & \n    (insurance['age'] > 45),\n    1, 0\n)\n\ninsurance['bmi_smoker_continuous'] = np.where(\n    insurance['smoker'] == 'yes',\n    insurance['bmi'],\n    0\n)\ninsurance['age_sq'] = insurance['age'] ** 2\n\ninsurance['bmi_sq'] = insurance['bmi'] ** 2\ninsurance['age_bmi_interaction'] = insurance['age'] * insurance['bmi']\ninsurance['sex_smoker_interaction'] = np.where(\n    (insurance['smoker'] == 'yes') & (insurance['sex'] == 'male'),\n    1, 0\n)\ninsurance['non_smoker_high_risk'] = np.where(\n    (insurance['smoker'] == 'no') & \n    (insurance['bmi_encoded'].isin([2, 3])) & \n    (insurance['age'] > 45),\n    1, 0\n)\n\ninsurance['log_charges'] = np.log(insurance['charges'])","metadata":{"_uuid":"9df74762-1457-43e7-9e42-c6571552f25a","_cell_guid":"f9ba1ef7-b126-4a56-8ddd-4de1b1f32ba9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-19T13:38:27.619906Z","iopub.execute_input":"2025-11-19T13:38:27.620241Z","iopub.status.idle":"2025-11-19T13:38:27.650866Z","shell.execute_reply.started":"2025-11-19T13:38:27.620213Z","shell.execute_reply":"2025-11-19T13:38:27.649506Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"insurance.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:38:27.652009Z","iopub.execute_input":"2025-11-19T13:38:27.652261Z","iopub.status.idle":"2025-11-19T13:38:27.684417Z","shell.execute_reply.started":"2025-11-19T13:38:27.65224Z","shell.execute_reply":"2025-11-19T13:38:27.683454Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Setup & Preprocessing Initialization\n\nTahap ini bertujuan untuk mempersiapkan data latih dan menginisialisasi komponen *preprocessing* menggunakan fungsi utilitas modular yang telah kita buat sebelumnya.\n\n### 1. Feature Selection & Target Definition\n\nKita memisahkan kolom menjadi fitur (X) dan target (y) secara otomatis agar prosesnya dinamis:\n\n* **`DROP_COLS`**: Mendefinisikan kolom target (`charges`) dan variannya (`log_charges`) untuk dikecualikan dari daftar fitur agar tidak terjadi kebocoran data (*data leakage*).\n* **`NUM_FEATS` & `CAT_FEATS`**: Menggunakan `select_dtypes` untuk memisahkan nama kolom numerik dan kategorikal secara otomatis, dikurangi kolom yang ada di `DROP_COLS`.\n* **`X` & `y`**:\n    * `X`: Menggabungkan daftar fitur numerik dan kategorikal sebagai input model.\n    * `y`: Menggunakan `charges` (nilai asli) sebagai target prediksi.\n\n### 2. Preprocessor Initialization\n\n* **`my_preprocessor`**: Kita memanggil fungsi utilitas **`get_preprocessor(NUM_FEATS, CAT_FEATS)`** yang telah kita definisikan di bagian *Utility Functions*.\n* Fungsi ini secara otomatis membungkus logika transformasi standar:\n    * **Numerik**: Diterapkan **`StandardScaler`** untuk menstandarisasi skala data.\n    * **Kategorikal**: Diterapkan **`OneHotEncoder`** untuk mengubah data teks menjadi format numerik biner.","metadata":{"_uuid":"08743edb-c9e6-48c1-923d-7054789f5f8a","_cell_guid":"9914dacc-4bf9-4c34-8cd5-b016140a0c03","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Setup Data\nDROP_COLS = ['log_charges', 'charges'] \nNUM_FEATS = insurance.select_dtypes(include=np.number).columns.difference(DROP_COLS)\nCAT_FEATS = insurance.select_dtypes(include='object').columns.difference(DROP_COLS)\n\nX = insurance[list(NUM_FEATS) + list(CAT_FEATS)]\ny = insurance['charges']\n\n# preprocessor\nmy_preprocessor = get_preprocessor(NUM_FEATS, CAT_FEATS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:38:27.685832Z","iopub.execute_input":"2025-11-19T13:38:27.686067Z","iopub.status.idle":"2025-11-19T13:38:27.704401Z","shell.execute_reply.started":"2025-11-19T13:38:27.686048Z","shell.execute_reply":"2025-11-19T13:38:27.703425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training & Optimization Phase\n\nPada tahap ini, kita menjalankan proses pelatihan model secara sistematis menggunakan *workflow* modular yang telah kita bangun. Kita akan membandingkan dua algoritma *Gradient Boosting* terpopuler: **XGBoost** dan **LightGBM**.\n\nSetiap model akan melalui dua fase utama: **Tuning** dan **Evaluation**.\n\n### 1. Phase A: Hyperparameter Tuning (`tune_model`)\nKita menggunakan fungsi utilitas `tune_model` yang memanfaatkan **Optuna** untuk mencari kombinasi parameter terbaik secara otomatis.\n\n* **Strategi Pencarian:** Fungsi ini memanggil `get_xgb_search_space` atau `get_lgbm_search_space` untuk menentukan rentang parameter yang akan diuji (misalnya: `learning_rate`, `max_depth`, `num_leaves`).\n* **Metode Validasi:** Menggunakan **3-Fold Cross-Validation** (seperti yang didefinisikan di dalam fungsi `tune_model`) untuk mempercepat proses pencarian tanpa mengorbankan validitas data.\n* **Optimasi:** Optuna akan menjalankan **100 trials**, mencoba meminimalkan skor *Mean Absolute Error* (MAE).\n\n### 2. Phase B: Robust Evaluation (`evaluate_final_model`)\nSetelah parameter terbaik (`best_params`) ditemukan, kita tidak langsung mempercayai hasil tuning tersebut begitu saja. Kita memvalidasinya ulang menggunakan fungsi `evaluate_final_model`.\n\n* **Pipeline Final:** Parameter terbaik disuntikkan ke dalam model baru dan dibungkus kembali dengan *preprocessor*.\n* **Evaluasi Ketat:** Kita meningkatkan validasi menjadi **5-Fold Cross-Validation**. Ini memberikan estimasi performa yang lebih stabil dan tidak bias dibandingkan saat fase tuning.\n* **Metrik Lengkap:** Fungsi ini akan mencetak skor rata-rata untuk **R2** (daya jelaskan model), **MAE** (kesalahan absolut), dan **RMSE** (kesalahan kuadratik) untuk memastikan model benar-benar robust sebelum digunakan.","metadata":{}},{"cell_type":"code","source":"# === 1. XGBoost Workflow ===\n# Step A: Tune\nxgb_best_params = tune_model(\n    model_cls=XGBRegressor, \n    search_space_func=get_xgb_search_space, \n    X=X, y=y, preprocessor=my_preprocessor, \n    n_trials=100, study_name=\"XGBoost\"\n)\n\n# Step B: Evaluate\nxgb_pipeline = evaluate_final_model(XGBRegressor, xgb_best_params, X, y, my_preprocessor)\n\n\n# === 2. LightGBM Workflow ===\n# Step A: Tune\nlgbm_best_params = tune_model(\n    model_cls=LGBMRegressor, \n    search_space_func=get_lgbm_search_space, \n    X=X, y=y, preprocessor=my_preprocessor, \n    n_trials=100, study_name=\"LightGBM\"\n)\n\n# Step B: Evaluate\nlgbm_pipeline = evaluate_final_model(LGBMRegressor, lgbm_best_params, X, y, my_preprocessor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:38:27.705297Z","iopub.execute_input":"2025-11-19T13:38:27.705618Z","iopub.status.idle":"2025-11-19T13:40:31.27517Z","shell.execute_reply.started":"2025-11-19T13:38:27.705585Z","shell.execute_reply":"2025-11-19T13:40:31.274461Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation & Interpretation\n\nBerdasarkan hasil *Hyperparameter Tuning* dan *5-Fold Cross-Validation*, berikut adalah analisis performa model prediksi biaya kesehatan (*charges*):\n\n### 1. Perbandingan Model (XGBoost vs LightGBM)\n\n| Metric | XGBoost (Winner) ðŸ† | LightGBM | Selisih |\n| :--- | :--- | :--- | :--- |\n| **R2 Score** | **0.8584** | 0.8516 | XGBoost +0.68% |\n| **MAE** | **`$2,454.01`** | `$2,563.73` | XGBoost lebih akurat ~`$110` |\n| **RMSE** | **`$4,500.18`** | `$4,608.39` | XGBoost lebih stabil terhadap *outlier* |\n\n**Kesimpulan Teknis:**\n**XGBoost** terbukti lebih superior dibandingkan LightGBM di semua metrik pengujian. Model ini mampu menjelaskan **85.84%** variabilitas dari biaya klaim kesehatan, yang merupakan angka yang sangat solid untuk data asuransi yang memiliki varians alami yang tinggi.\n\n### 2. Interpretasi Metrik dalam Konteks Asuransi\n\n#### a. R2 Score (0.8584) - \"Explanatory Power\"\nModel kita berhasil menangkap pola utama risiko. Artinya, fitur-fitur yang kita rekayasa (seperti `smoker_obese_interaction`, `bmi_encoded`, dll) sangat efektif. Sisa ~14% variabilitas yang tidak tertangkap kemungkinan adalah faktor acak (kecelakaan, penyakit mendadak tak terduga) atau variabel yang tidak tersedia di dataset (riwayat medis keluarga, genetik).\n\n#### b. MAE (Mean Absolute Error): ~`$2,454`\n* **Artinya:** Secara rata-rata, prediksi premi/klaim kita meleset (kurang atau lebih) sebesar **2,454 USD** dari tagihan aslinya.\n* **Implikasi Bisnis:** Dalam penetapan harga (*pricing*), ini adalah rata-rata \"ketidakpastian\" per polis. Jika margin keuntungan asuransi per orang lebih kecil dari angka ini, perusahaan berisiko rugi pada level individu, namun bisa tertutupi oleh hukum bilangan besar (*law of large numbers*) jika agregatnya akurat.\n\n#### c. Gap antara MAE (`$2,454`) dan RMSE (`$4,500`)\n* RMSE jauh lebih tinggi daripada MAE (hampir 2x lipat).\n* **Artinya:** Data klaim asuransi memiliki **Outlier Ekstrem** (Klaim Katastropik). Model terkadang melakukan kesalahan prediksi yang *sangat besar* pada kasus-kasus mahal (misalnya: memprediksi **`$10k`** padahal klaim aslinya **`$50k`**).\n* **Risiko:** Model mungkin sedikit *underfitting* pada kasus penyakit berat yang biayanya meledak, yang mana wajar karena kasus tersebut jarang terjadi (*sparse data*).\n\n### 3. Stabilitas Model (Cross-Validation Analysis)\nMelihat hasil per-*fold* pada XGBoost:\n* **Best Case (Fold 1):** R2 0.90 (Sangat Akurat)\n* **Worst Case (Fold 2):** R2 0.82 (Cukup Akurat)\n* **Analisis:** Variasi antara 0.82 hingga 0.90 menunjukkan bahwa model cukup stabil, namun ada sebagian kecil data (Fold 2) yang \"sulit\" diprediksi. Ini mungkin berisi kelompok pasien dengan profil aneh (misal: muda, tidak merokok, tapi klaimnya tinggi karena penyakit bawaan).\n\n### âœ… Rekomendasi Akhir\nKita akan menggunakan **XGBoost** sebagai model final (*champion model*). Performanya secara konsisten lebih baik dalam meminimalkan risiko kesalahan harga (*pricing error*) dibandingkan LightGBM.","metadata":{"_uuid":"c75b9733-260b-48f4-ad4b-2f4229b9564c","_cell_guid":"39216f6a-d75d-437a-850f-71f4df66b072","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Visualisasi XGBoost\ndf_imp_xgb = plot_feature_importance(xgb_pipeline, X, y, model_name=\"XGBoost\")\n\n# Visualisasi LightGBM\ndf_imp_lgbm = plot_feature_importance(lgbm_pipeline, X, y, model_name=\"LightGBM\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:40:31.276394Z","iopub.execute_input":"2025-11-19T13:40:31.277083Z","iopub.status.idle":"2025-11-19T13:40:32.951049Z","shell.execute_reply.started":"2025-11-19T13:40:31.277058Z","shell.execute_reply":"2025-11-19T13:40:32.950082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance Analysis\n\nSetelah melatih kedua model, kita meninjau fitur mana yang paling berkontribusi terhadap prediksi biaya asuransi. Visualisasi di atas menunjukkan perbedaan strategi yang mencolok antara XGBoost dan LightGBM.\n\n### 1. XGBoost: The \"Interaction Hunter\"\nXGBoost (Model Pemenang kita) sangat menyukai fitur-fitur hasil rekayasa (*Feature Engineering*) yang kita buat.\n\n* **Dominasi Mutlak `smoker_obese_interaction`**:\n    Fitur ini menjadi juara tak terbantahkan dengan skor *importance* melebihi **0.5**. Ini membuktikan bahwa hipotesis kita benar: Risiko (dan biaya) tertinggi bukan hanya karena merokok atau obesitas secara terpisah, melainkan **kombinasi keduanya**. XGBoost langsung mengenali bahwa jika seseorang perokok *dan* obesitas, biayanya akan meledak.\n* **`bmi_smoker_continuous`**:\n    Fitur terpenting kedua. Ini menegaskan bahwa bagi perokok, setiap kenaikan angka BMI berdampak sangat besar pada biaya.\n* **Kesimpulan XGBoost**: Model ini bekerja sangat efektif karena kita membantunya dengan fitur interaksi. Ia \"malas\" melihat fitur dasar (`bmi` atau `children` sendirian) karena fitur turunan kita sudah memberikan sinyal yang jauh lebih kuat.\n\n### 2. LightGBM: The \"Raw Data\" Analyst\nLightGBM memiliki pendekatan yang sangat berbeda. Ia kurang tertarik pada fitur interaksi biner kita dan lebih memilih variabel kontinu asli.\n\n* **Fokus pada `bmi` dan `age`**:\n    LightGBM menempatkan `bmi` (murni) dan `age_bmi_interaction` sebagai fitur teratas. Ia mencoba memecah pohon keputusan berdasarkan angka BMI secara mendetail, bukan berdasarkan kategori \"Obesitas\" yang kita buat.\n* **Mengabaikan Flag Perokok**:\n    Yang mengejutkan, fitur terkait status perokok (`smoker_yes`, `smoker_obese_interaction`) justru dianggap tidak terlalu penting dibandingkan variabel demografis kontinu.\n* **Kelemahan**: Strategi ini mungkin menjadi alasan mengapa LightGBM kalah akurat dari XGBoost. Dalam asuransi kesehatan, status perokok adalah faktor pembeda tarif yang tegas (diskrit), bukan gradasi halus. Dengan terlalu fokus pada BMI kontinu, LightGBM mungkin kehilangan ketegasan pola \"High Risk Group\" yang ditangkap sempurna oleh XGBoost.\n\n### Insight & Conclusion\n\n1.  **Validasi Feature Engineering**: Kemenangan XGBoost adalah bukti bahwa proses *feature engineering* kita (terutama pembuatan `smoker_obese_interaction`) **sangat sukses**. Tanpa fitur ini, model mungkin akan kesulitan menangkap lonjakan biaya pada kelompok risiko tinggi.\n2.  **Faktor Risiko Utama**: Bagi perusahaan asuransi, data ini berteriak satu hal: **Saring ketat nasabah yang Merokok dan Obesitas.** Mereka adalah penyumbang varians klaim terbesar.\n3.  **Rekomendasi**: Pertahankan fitur interaksi tersebut saat *deployment* model, karena itulah \"otak\" utama dari prediksi akurat XGBoost.","metadata":{}},{"cell_type":"markdown","source":"## Deep Learning","metadata":{}},{"cell_type":"code","source":"# Run DNN Workflow\ndnn_results = run_dnn_cv(\n    X=X, \n    y=y, \n    num_cols=NUM_FEATS, \n    cat_cols=CAT_FEATS, \n    n_splits=5, \n    epochs=100, \n    batch_size=32\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:40:32.953122Z","iopub.execute_input":"2025-11-19T13:40:32.953459Z","iopub.status.idle":"2025-11-19T13:41:41.25734Z","shell.execute_reply.started":"2025-11-19T13:40:32.953436Z","shell.execute_reply":"2025-11-19T13:41:41.256082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Deep Neural Network (DNN) Evaluation\n\nKami mencoba pendekatan *Deep Learning* menggunakan arsitektur Neural Network sederhana. Berikut adalah hasil evaluasinya:\n\n### 1. Ringkasan Performa\n* **R2 Score**: **0.8446** (Lebih rendah dari XGBoost 0.8584)\n* **MAE**: **`$2,659.17`** (Error lebih tinggi ~`$200` dibanding XGBoost)\n* **RMSE**: **`$4,702.46`**\n\n### 2. Mengapa DNN Tidak Optimal di Sini?\n\nMeskipun skor R2 sebesar 0.84 menunjukkan model ini \"layak\", namun performanya **kalah konsisten** dibandingkan XGBoost. Ada beberapa alasan teknis mengapa DNN kesulitan mengalahkan *Gradient Boosting* pada kasus ini:\n\n#### a. Kutukan Data Tabular (*Tabular Data Struggle*)\nDeep Learning sangat superior untuk data tidak terstruktur (gambar, teks, suara). Namun, untuk **data tabular** (kolom & baris) dengan jumlah baris yang terbatas (dataset kita < 2000 baris), algoritma berbasis pohon (*Tree-based*) seperti XGBoost hampir selalu menang.\n* **XGBoost** bekerja dengan membuat \"pemisah\" tegas (misal: Jika `smoker=yes` DAN `bmi > 30`, maka biaya tinggi). Ini sangat cocok dengan pola asuransi.\n* **DNN** mencoba mencari fungsi matematika yang mulus (*smooth function*) melalui perkalian matriks (bobot & bias). DNN kesulitan menangkap \"lompatan\" risiko yang tajam dan diskrit pada aturan bisnis asuransi.\n\n#### b. Instabilitas Antar Fold\nPerhatikan perbedaan drastis antara **Fold 1 (R2 0.89)** dan **Fold 2 (R2 0.80)**.\nJarak performa yang jauh ini menunjukkan bahwa DNN agak **tidak stabil** dan sensitif terhadap pembagian data. Ia mungkin mengalami *overfitting* pada fold tertentu dan gagal menggeneralisasi pola pada fold lainnya.\n\n### 3. Kesimpulan\nPenggunaan Deep Learning pada kasus ini tergolong **\"Overkill\" namun \"Underperforming\"**. Kompleksitas model bertambah, waktu training lebih lama, namun akurasinya justru turun.","metadata":{}},{"cell_type":"code","source":"# === Visualisasi Feature Importance DNN ===\nfinal_dnn_model, X_dnn_proc, dnn_feats = train_final_dnn(X, y, NUM_FEATS, CAT_FEATS)\ndf_imp_dnn = explain_dnn_feature_importance(final_dnn_model, X_dnn_proc, y, dnn_feats)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:41:41.258777Z","iopub.execute_input":"2025-11-19T13:41:41.259135Z","iopub.status.idle":"2025-11-19T13:42:06.507011Z","shell.execute_reply.started":"2025-11-19T13:41:41.259102Z","shell.execute_reply":"2025-11-19T13:42:06.505821Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### DNN Feature Importance\n\nVisualisasi *Permutation Importance* pada DNN menunjukkan pola unik yang berbeda dari model *Tree-based* sebelumnya:\n\n1.  **Dominasi `bmi_smoker_continuous`**:\n    Berbeda dengan XGBoost yang menyukai fitur \"aturan kaku\" (`smoker_obese_interaction` berupa 0/1), DNN justru sangat bergantung pada **`bmi_smoker_continuous`**. Ini masuk akal karena Neural Network bekerja berbasis fungsi matematika dan gradien; ia lebih mudah belajar dari **angka kontinu** yang memiliki gradasi nilai daripada kategori biner yang patah-patah.\n\n2.  **Menangkap Pola Non-Linier (`age_sq`)**:\n    Fitur **`age_sq`** (usia kuadrat) muncul sebagai fitur terpenting kedua. Ini menunjukkan bahwa DNN sedang berusaha memodelkan kurva biaya yang melengkung seiring bertambahnya usia (fungsi eksponensial), sesuatu yang tidak terlalu diprioritaskan oleh model *Tree* yang hanya melakukan pemisahan data (*splitting*).","metadata":{}},{"cell_type":"markdown","source":"# ðŸ¥Š 6. Final Evaluation and Ensembling","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\n\n# --- 1. Setup Ulang Pipeline ML ---\n# Ambil regressor terbaik yang sudah dituning\nxgb_final = xgb_pipeline.named_steps['regressor']\nlgbm_final = lgbm_pipeline.named_steps['regressor']\n\nxgb_final.set_params(n_jobs=1)\nlgbm_final.set_params(n_jobs=1)\n\n# Buat Ensemble Voting\nvoting_reg = VotingRegressor(estimators=[('xgb', xgb_final), ('lgbm', lgbm_final)], n_jobs=1)\n\n# Dictionary Model ML\nml_models = {\n    'XGBoost': xgb_pipeline,\n    'LightGBM': lgbm_pipeline,\n    'Ensemble': Pipeline([('preprocessor', my_preprocessor), ('voting', voting_reg)])\n}\n\n# --- Hitung Skor ML & Gabungkan dengan DNN ---\nprint(\"ðŸ¥Š FINAL BATTLE: Machine Learning vs Deep Learning\")\nprint(\"-\" * 50)\n\nleaderboard = []\n\n# Loop ML Models\nscoring = {'r2': 'r2', 'mae': 'neg_mean_absolute_error', 'rmse': 'neg_root_mean_squared_error'}\n\nfor name, pipe in ml_models.items():\n    print(f\"Running CV for {name}...\")\n    cv = cross_validate(pipe, X, y, cv=5, scoring=scoring, n_jobs=1)\n    leaderboard.append({\n        'Model': name,\n        'R2 Score': cv['test_r2'].mean(),\n        'MAE': -cv['test_mae'].mean(),\n        'RMSE': -cv['test_rmse'].mean()\n    })\n\n# Masukkan Hasil DNN (Dari variabel dnn_results sebelumnya)\nleaderboard.append({\n    'Model': 'Deep Learning (DNN)',\n    'R2 Score': dnn_results['R2 Score'],\n    'MAE': dnn_results['MAE'],\n    'RMSE': dnn_results['RMSE']\n})\n\n# --- Tampilkan Leaderboard Akhir ---\ncomparison_df = pd.DataFrame(leaderboard).set_index('Model').sort_values(by='MAE')\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ðŸ† FINAL LEADERBOARD (Sorted by MAE)\")\nprint(\"=\"*60)\nprint(comparison_df)\nprint(\"-\" * 60)\n\nwinner = comparison_df.index[0]\nprint(f\"ðŸŽ‰ AND THE WINNER IS: {winner} ehehehe!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T13:42:06.508136Z","iopub.execute_input":"2025-11-19T13:42:06.508503Z","iopub.status.idle":"2025-11-19T13:42:12.240499Z","shell.execute_reply.started":"2025-11-19T13:42:06.508464Z","shell.execute_reply":"2025-11-19T13:42:12.23936Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FINAL BATTLE ANALYSIS: Complexity vs. Efficiency\n\nKita telah mencapai tahap akhir. Setelah mengadu model tunggal (*Single Models*) melawan model gabungan (*Ensemble*) dan Deep Learning, berikut adalah kesimpulannya:\n\n### 1. The Champion: XGBoost (Solo) ðŸ†\n* **MAE: `$2,430.26`**\n* **Analisis:** Secara mengejutkan (atau mungkin tidak), **XGBoost murni** keluar sebagai pemenang. Model ini bekerja sendirian tanpa perlu digabung dengan model lain.\n* **Kenapa menang?** XGBoost sudah sangat optimal dalam menangkap pola diskrit (patah-patah) pada data asuransi berkat *Feature Engineering* interaksi kita. Ia tidak membutuhkan \"bantuan\" dari model lain untuk memperbaiki prediksinya.\n\n### 2. The Ensemble Dilemma (Voting Regressor)\n* **MAE: `$2,459.31`** (Juara 2)\n* **Strategi:** Kita menggunakan `VotingRegressor` yang mengambil rata-rata prediksi dari XGBoost dan LightGBM.\n* **Interpretasi Hasil:**\n    Hasil Ensemble berada tepat **di tengah-tengah** antara XGBoost (terbaik) dan LightGBM (terburuk).\n    * Ibaratnya: Jika Anda mencampur Jus Mangga murni yang sangat manis (XGBoost) dengan air tawar (LightGBM), hasilnya memang masih manis, tapi tidak semanis jus murninya.\n    * **Pelajaran:** Ensembling bekerja paling baik jika model-model penyusunnya memiliki performa yang **setara** atau jika mereka melakukan kesalahan pada jenis data yang **berbeda** (saling melengkapi). Di sini, LightGBM hanya menarik turun rata-rata performa XGBoost.\n\n### 3. Deep Learning (DNN)\n* **MAE: `$2,659.17`** (Posisi Terakhir)\n* **Analisis:** DNN konsisten berada di posisi terakhir dengan selisih error sekitar **`$230`** lebih mahal dibanding XGBoost. Ini mengonfirmasi bahwa untuk dataset tabular berukuran kecil (<2000 baris) dengan pola aturan bisnis yang tegas, *Neural Networks* bukanlah solusi yang efisien.\n\n### Final Verdict\nKompleksitas tidak selalu menjamin akurasi. Kita tidak perlu menggunakan *Deep Learning* yang berat atau *Ensemble* yang rumit. Model **XGBoost** tunggal dengan *Feature Engineering* yang cerdas sudah cukup untuk menjadi solusi terbaik (*State-of-the-Art*) untuk permasalahan prediksi biaya asuransi ini.","metadata":{}},{"cell_type":"markdown","source":"# ðŸ 7. Executive Summary & Conclusion\n\nBerdasarkan rangkaian eksperimen mulai dari *preprocessing*, *feature engineering*, hingga *model comparison* (Machine Learning vs Deep Learning), berikut adalah kesimpulan strategis dari proyek ini:\n\n### 1. Faktor Determinan Klaim Asuransi (Key Risk Drivers)\nAnalisis *Feature Importance* model pemenang (XGBoost) mengungkap fakta bahwa struktur biaya asuransi tidak dipengaruhi oleh faktor tunggal, melainkan **interaksi antar faktor risiko**:\n\n* **The \"Double Whammy\" Effect (Perokok + Obesitas)**\n    Ini adalah faktor paling mematikan dan paling mahal. Fitur `smoker_obese_interaction` mendominasi prediksi. Nasabah yang merokok **DAN** memiliki BMI tinggi (Obesitas) memiliki profil risiko yang jauh lebih tinggi secara eksponensial dibandingkan penjumlahan risiko merokok saja ditambah risiko obesitas saja.\n* **Gaya Hidup > Demografi Murni**\n    Faktor gaya hidup (BMI, Status Merokok) memiliki bobot prediksi yang lebih besar dibandingkan faktor demografi statis seperti `region` (wilayah tinggal) atau `sex` (jenis kelamin).\n* **Penuaan (Aging)**\n    Usia (`age` dan `age_sq`) tetap menjadi faktor fundamental. Biaya naik seiring bertambahnya umur, dan kenaikannya cenderung melengkung (non-linier), terutama di usia tua.\n\n### 2. Penilaian Kelayakan Model (Model Eligibility)\n\nApakah model ini **LAYAK (Eligible)** untuk digunakan dalam sistem penetapan premi asuransi nyata?\n\n**Status: âœ… ELIGIBLE WITH GUARDRAILS (Layak dengan Pengawasan)**\n\n* **Alasan Layak:**\n    * **Akurasi Tinggi (R2 ~86%):** Model mampu menjelaskan 86% variasi biaya. Dalam dunia aktuaria di mana perilaku manusia dan penyakit sangat acak, angka ini sangat solid untuk penetapan tarif dasar (*base rate*).\n    * **Diskriminasi Risiko yang Tajam:** Model sangat jago membedakan mana nasabah \"Murah\" dan nasabah \"Mahal\" berkat *feature engineering* yang kuat.\n\n* **Catatan & Risiko (Guardrails):**\n    * **Margin Error (`$2,430`):** Rata-rata prediksi meleset sekitar `$2,430`. Perusahaan asuransi harus menambahkan **Risk Loading** (biaya cadangan) sebesar nilai ini ke dalam premi agar tidak rugi.\n    * **Isu Outlier (RMSE Tinggi):** Karena RMSE (`$4,468`) hampir 2x lipat MAE, model terkadang masih *under-predict* pada kasus penyakit katastropik (klaim super besar).\n    * **Saran Implementasi:** Gunakan prediksi model ini sebagai **acuan dasar (benchmark)**, namun untuk kasus dengan prediksi biaya sangat tinggi, tetap perlukan tinjauan manual (*human underwriter review*).","metadata":{}}]}